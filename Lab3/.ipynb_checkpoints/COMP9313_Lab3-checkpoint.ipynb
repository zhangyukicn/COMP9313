{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3\n",
    "In this lab, we will use PySpark ML (pyspark.ml) and PySpark SQL (pyspark.sql) to impletement different classifiers for the document classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to install numpy to execute this code correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"lab3\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|category|            descript|\n",
      "+--------+--------------------+\n",
      "|    MISC|I've been there t...|\n",
      "|    REST|Stay away from th...|\n",
      "|    REST|Wow over 100 beer...|\n",
      "|    MISC|Having been a lon...|\n",
      "|    MISC|This is a consist...|\n",
      "|    REST|I ate here a week...|\n",
      "|    MISC|First of all Dal ...|\n",
      "|    REST|Great food at REA...|\n",
      "|    REST|While there are p...|\n",
      "|    MISC|My first encounte...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "---------\n",
      "Schema of train_data:\n",
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- descript: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = spark.read.load(\"Lab3train.csv\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")\n",
    "test_data = spark.read.load(\"Lab3test.csv\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")\n",
    "train_data.show(10)\n",
    "print('---------\\nSchema of train_data:')\n",
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Feature Generation\n",
    "We are going to evaluate the performance of different models and choose the best one. So, we don't include the classifier in the pipeline.\n",
    "\n",
    "We tokenize each document into a sequence of tokens and generate features as the frequency of tokens. And, transform the label (e.g., category) into an indexed vector.\n",
    "\n",
    "We only keep those columns that will be used by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white space expression tokenizer\n",
    "word_tokenizer = Tokenizer(inputCol=\"descript\", outputCol=\"words\")\n",
    "\n",
    "# bag of words count\n",
    "count_vectors = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "\n",
    "# label indexer\n",
    "label_maker = StringIndexer(inputCol = \"category\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selector(Transformer):\n",
    "    def __init__(self, outputCols=['features', 'label']):\n",
    "        self.outputCols=outputCols\n",
    "        \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df.select(*self.outputCols)\n",
    "\n",
    "selector = Selector(outputCols = ['features', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the pipeline\n",
    "pipeline = Pipeline(stages=[word_tokenizer, count_vectors, label_maker, selector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(5421,[1,18,31,39...|  1.0|\n",
      "|(5421,[0,1,15,20,...|  0.0|\n",
      "|(5421,[3,109,556,...|  0.0|\n",
      "|(5421,[1,2,3,5,6,...|  1.0|\n",
      "|(5421,[2,3,4,8,11...|  1.0|\n",
      "|(5421,[1,2,5,25,4...|  0.0|\n",
      "|(5421,[7,40,142,1...|  1.0|\n",
      "|(5421,[8,13,19,25...|  0.0|\n",
      "|(5421,[2,3,7,8,21...|  0.0|\n",
      "|(5421,[2,16,22,49...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline using train_data.\n",
    "fitted_pipeline = pipeline.fit(train_data)\n",
    "\n",
    "\n",
    "# Transform the train_data using fitted pipeline\n",
    "training_set = fitted_pipeline.transform(train_data)\n",
    "training_set.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform cross validation, we need to generate a random group id for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|            features|label|group|\n",
      "+--------------------+-----+-----+\n",
      "|(5421,[1,18,31,39...|  1.0|    3|\n",
      "|(5421,[0,1,15,20,...|  0.0|    3|\n",
      "|(5421,[3,109,556,...|  0.0|    2|\n",
      "|(5421,[1,2,3,5,6,...|  1.0|    4|\n",
      "|(5421,[2,3,4,8,11...|  1.0|    2|\n",
      "|(5421,[1,2,5,25,4...|  0.0|    3|\n",
      "|(5421,[7,40,142,1...|  1.0|    0|\n",
      "|(5421,[8,13,19,25...|  0.0|    0|\n",
      "|(5421,[2,3,7,8,21...|  0.0|    4|\n",
      "|(5421,[2,16,22,49...|  1.0|    2|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.withColumn('group', (rand()*5).cast(IntegerType()))\n",
    "training_set.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction\n",
    "We construct and evaluate three different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "bow_lr = LogisticRegression(featuresCol='features', labelCol='label', predictionCol='lr_prediction',\n",
    "                            maxIter=20, regParam=1., elasticNetParam=0)\n",
    "\n",
    "#bow_lr_model = bow_lr.fit(train_dataset)\n",
    "#bow_lr_predictions = bow_lr_model.transform(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "bow_nb = NaiveBayes(featuresCol='features', labelCol='label', predictionCol='nb_prediction')\n",
    "#bow_nb_model = bow_nb.fit(train_dataset)\n",
    "#bow_nb_predictions = bow_nb_model.transform(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "bow_svm = LinearSVC(featuresCol='features', labelCol='label', predictionCol='svm_prediction')\n",
    "#bow_svm_model = bow_svm.fit(train_dataset)\n",
    "#bow_svm_predictions = bow_svm_model.transform(dev_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",metricName='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_f1 = []\n",
    "nb_f1 = []\n",
    "svm_f1 = []\n",
    "for i in range(5):\n",
    "    condition = training_set['group'] == i\n",
    "    c_train = training_set.filter(~condition).cache()\n",
    "    c_test = training_set.filter(condition).cache()\n",
    "    \n",
    "    lr_model = bow_lr.fit(c_train)\n",
    "    lr_pred = lr_model.transform(c_test)\n",
    "    lr_f1.append(evaluator.evaluate(lr_pred, {evaluator.predictionCol:'lr_prediction'}))\n",
    "    \n",
    "    nb_model = bow_nb.fit(c_train)\n",
    "    nb_pred = nb_model.transform(c_test)\n",
    "    nb_f1.append(evaluator.evaluate(nb_pred, {evaluator.predictionCol:'nb_prediction'}))\n",
    "    \n",
    "    svm_model = bow_svm.fit(c_train)\n",
    "    svm_pred = svm_model.transform(c_test)\n",
    "    svm_f1.append(evaluator.evaluate(svm_pred, {evaluator.predictionCol:'svm_prediction'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Performance of LR: {}'.format(sum(lr_f1)/len(lr_f1)))\n",
    "print('Performance of NB: {}'.format(sum(nb_f1)/len(nb_f1)))\n",
    "print('Performance of SVM: {}'.format(sum(svm_f1)/len(svm_f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, SVM has the best performance among all three models. Hence we will use SVM to train the classifier on the whole training_set, and evaluate it on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the pipeline on the testing set\n",
    "test_set = fitted_pipeline.transform(test_data)\n",
    "test_set.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = bow_svm.fit(training_set)\n",
    "svm_pred = svm_model.transform(test_set)\n",
    "print('Performance on test data: {}'.format(evaluator.evaluate(svm_pred, {evaluator.predictionCol:'svm_prediction'})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP9313",
   "language": "python",
   "name": "comp9313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
